{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn\n",
    "**Scikit-learn** is an open source Python library that implements a range of **machine learning, preprocessing, cross-validation and visualization** algorithms using a unified interface\n",
    "\n",
    "**Contents**   \n",
    "[1. Basic Example](#1)  \n",
    "&nbsp;&nbsp;&nbsp;[1.1 Import modules](#1.1)  \n",
    "&nbsp;&nbsp;&nbsp;[1.2 Import data](#1.2)  \n",
    "&nbsp;&nbsp;&nbsp;[1.3 Set features and target](#1.3)  \n",
    "&nbsp;&nbsp;&nbsp;[1.4 Split train and test](#1.4)  \n",
    "&nbsp;&nbsp;&nbsp;[1.5 Create model](#1.5)  \n",
    "&nbsp;&nbsp;&nbsp;[1.6 Train model](#1.6)  \n",
    "&nbsp;&nbsp;&nbsp;[1.7 Predict test](#1.7)  \n",
    "&nbsp;&nbsp;&nbsp;[1.8 Calculate accuracy](#1.8)\n",
    "[2. Load data](#2)  \n",
    "[3. Train and Test Data](#3)  \n",
    "[4. Preprocessing Data](#4)  \n",
    "&nbsp;&nbsp;&nbsp;[4.1 Standardization](#2.1)  \n",
    "&nbsp;&nbsp;&nbsp;[4.2 Normalization](#2.2)  \n",
    "&nbsp;&nbsp;&nbsp;[4.3 Binarization](#2.3)  \n",
    "&nbsp;&nbsp;&nbsp;[4.4 Encoding Categorical Features](#2.4)  \n",
    "&nbsp;&nbsp;&nbsp;[4.5 Imputing Missing Values](#2.5)  \n",
    "&nbsp;&nbsp;&nbsp;[4.6 Generating Polynomial Features](#2.6)    \n",
    "[5. Create Model](#5)  \n",
    "&nbsp;&nbsp;&nbsp;[5.1 Supervised Learning Estimators](#5.1)  \n",
    "&nbsp;&nbsp;&nbsp;[4.6 Generating Polynomial Features](#5.2)  \n",
    "[6. Fit Model](#6)  \n",
    "[7. Prediction](#7)  \n",
    "[8. Evaluate Model Performance](#8)  \n",
    "&nbsp;&nbsp;&nbsp;[8.1 Classification Metrics](#8.1)  \n",
    "&nbsp;&nbsp;&nbsp;[8.2 Regression Metrics](#8.2)  \n",
    "&nbsp;&nbsp;&nbsp;[8.3 Clustering Metrics](#8.3)  \n",
    "&nbsp;&nbsp;&nbsp;[8.4 Cross-Validation](#8.4)  \n",
    "[9. Tune Model](#9)  \n",
    "&nbsp;&nbsp;&nbsp;[9.1 Grid Search](#9.1)  \n",
    "&nbsp;&nbsp;&nbsp;[9.2 Randomized Parameter Optimization](#9.2)  \n",
    "\n",
    "\n",
    "## <a id=\"1\">1. Basic Example </a>\n",
    "### <a id=\"1.1\">1.1 Import modules </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"1.2\">1.2 Import data </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"1.3\">1.3 Set features and target </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris.data[:,:2], iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"1.4\">1.4 Split train and test </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"1.4\">1.4 Preprocess data </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"1.5\">1.5 Create model </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier (n_neighbors = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"1.6\">1.6 Train model </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"1.7\">1.7 Predict test </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"1.8\">1.8 Calculate accuracy </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631578947368421"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"2\">2. Load data </a>\n",
    "Data need to be numeric and storec as NumPy arrays or SciPy sparse matrices. Other types that are convertible to numeric arrays, such as Pandas Dataframe, are also acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.random((10,5))\n",
    "y = np.array(['M','M','F','F','F','M','F','M','M','F','F','F'])\n",
    "X[X<0.7]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"3\">3. Train and Test Data </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris.data[:,:2], iris.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"4\">4. Preprocessing Data</a>\n",
    "The sklearn.preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators\n",
    "### <a id=\"4.1\">4.1 Standardization</a>\n",
    "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means: [ 0. -0.]\n",
      "Std dev: [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler =StandardScaler().fit(X_train)\n",
    "standarized_X = scaler.transform(X_train)\n",
    "standarized_X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Means:\",standarized_X.mean(axis=0).round())\n",
    "print(\"Std dev:\",standarized_X.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"4.2\">4.2 Normalization </a>\n",
    "Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.\n",
    "\n",
    "This assumption is the base of the Vector Space Model often used in text classification and clustering contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.32\n",
      "Max: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer().fit(X_train)\n",
    "normalized_X=scaler.transform(X_train)\n",
    "normalized_X_test=scaler.transform(X_test)\n",
    "\n",
    "print(\"Min:\", normalized_X.min().round(2))\n",
    "print(\"Max:\", normalized_X.max().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"4.3\">4.3 Binarization </a>\n",
    "Feature binarization is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "binarizer = Binarizer(threshold=0.0).fit(X)\n",
    "binary_X = binarizer.transform(X)\n",
    "print (binary_X.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"4.4\">4.4 Encoding Categorical Features </a>\n",
    "Encode labels with value between 0 and n_classes-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F' 'M' 'Q']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = np.array(['M','M','F','F','F','M','F','M','M','F','F','F','Q'])\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "enc = LabelEncoder()\n",
    "y = enc.fit_transform(y)\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"4.5\">4.5 Imputing Missing Values </a>\n",
    "Imputation transformer for completing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values=0,strategy='mean',axis=0)\n",
    "np.unique(np.isnan(imp.fit_transform(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"4.6\">4.6 Generating Polynomial Features </a>\n",
    "Generate polynomial and interaction features.\n",
    "\n",
    "Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(5)\n",
    "polyX = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"5\">5. Create Model </a>\n",
    "### <a id=\"5.1\">5.1 Supervised Learning Estimators </a>\n",
    "#### <a id=\"5.1.1\">5.1.1 Linear Regression </a>\n",
    "From the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) wrapped as a predictor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"5.1.2\">5.1.2 Support Vector Machines (SVM) </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"5.1.3\">5.1.3 Naive Bayes </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"5.1.4\">5.1.4 KNN </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"5.2\"> 5.2 Unsupervied Learning Estimators </a>\n",
    "#### <a id=\"5.2.1\"> 5.2.1 Principal Component Analysis (PCA) </a>\n",
    "Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"5.2.2\"> 5.2.2 K Means </a>\n",
    "K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans (n_clusters=3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"6\">6. Fit Model </a>\n",
    "### <a id=\"6.1\">6.1 Supervised learning</a>\n",
    "http://scikit-learn.org/stable/supervised_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lr.fit(X,y)\n",
    "# knn.fit(X_train, y_train)\n",
    "# svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"6.2\">6.2 Unsupervised Learning</a>\n",
    "http://scikit-learn.org/stable/unsupervised_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means.fit(X_train)\n",
    "pca_model = pca.fit_tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"7\">7. Prediction </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"8\">8. Evaluate Model Performance </a>\n",
    "### <a id=\"8.1\">8.1 Classification Metrics </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"8.2\">8.2 Regression Metrics </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"8.3\">8.3 Clustering Metrics </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"8.4\">8.4 Cross-Validation </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"9\">9. Tune Model </a>\n",
    "### <a id=\"9.1\">9.1 Grid Search </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"9.2\">9.2 Randomized Parameter Optimization </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
